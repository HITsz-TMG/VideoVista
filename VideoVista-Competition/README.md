
<div align="center">
  <img src="https://raw.githubusercontent.com/HITsz-TMG/VideoVista/main/VideoVista-CulturalLingo/asset/VideoVista_no_bg_s.png" alt="VideoVista Logo" width="300"/>
</div>

# HITSZ Lychee-VideoVista Evaluation Competition

[![arXiv](https://img.shields.io/badge/arXiv-2406.11303-b31b1b.svg)](https://arxiv.org/abs/2406.11303)
[![arXiv](https://img.shields.io/badge/arXiv-2504.17821-b31b1b.svg)](https://arxiv.org/abs/2504.17821)
[![ğŸ¤— Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-VideoVista-yellow)](https://huggingface.co/collections/Uni-MoE/videovista-67f397b7eddc81cb207228a2)
[![ğŸ¤— Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-VideoVista_CulturalLingo-yellow)](https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo)


If you appreciate our competition, please consider giving us a star â­ on GitHub to stay updated with the latest developments.

## ğŸ”¥ News

- **`2025.11.17`** ğŸš€ Competition registration is now open! Join us for the HITSZ Lychee Video Evaluation Competition. Fill out [**Team Registration Form**](https://docs.qq.com/form/page/DQ2NBbWF1cHpmWFJK#/fill) and [**Participants Registration Form**](https://docs.qq.com/form/page/DQ2JPenFKZFVJUW5y#/fill) to Register! **Attention:** In addition to filling out the Team Registration Form as a team, each participant must also fill out the Participants Registration Form individually.

## ğŸ† Competition

With the exponential growth of internet video data, video analysis technology based on multimodal large models is becoming a key force driving AI development. To systematically evaluate and push the boundaries of multimodal large models in video understanding, we are hosting the **HITSZ Lychee VideoVista Evaluation Competition**.

### ğŸ“‹ Competition Overview

This competition establishes a comprehensive evaluation framework using internet video clips and high-quality human-annotated QA pairs as core inputs. The framework systematically assesses model performance across multiple dimensions:

- **Perception Capability** - Object recognition, scene understanding
- **Cognitive & Reasoning Ability** - Logical, causal, and relational reasoning  
- **Cross-modal Fusion** - Integrating visual, textual, and temporal information
- **Cross-cultural & Temporal Understanding** - Cultural context and timeline comprehension
- **Scientific & Professional Domains** - Specialized knowledge in various fields

### ğŸ—“ï¸ Competition Schedule

| Stage | Time | Description |
|-------|------|-------------|
| **Competition Registration** | 2025-11-17 00:00 - 2026-1-10 23:59 | Competition launch; participants must register. |
| **Benchmark A Submission** | 2025-11-17 00:00 - 2026-1-17 23:59 | Participants submit their evaluation results of benchmark A by email. The score will be based on the highest score submitted by the team. |
| **Benchmark B Submission** | 2026-1-18 00:00 - 2026-1-31 23:59 | Participants submit their evaluation results of benchmark B by email. The score will be based on the highest score submitted by the team. |

### ğŸ‘¥ Participation Requirements

- **Open to**: Global individuals, universities, research institutions, enterprises
- **Team Size**: 1-5 members per team
- **Registration**: All participating members must fill out [**Team Registration Form**](https://docs.qq.com/form/page/DQ2NBbWF1cHpmWFJK#/fill) and [**Participants Registration Form**](https://docs.qq.com/form/page/DQ2JPenFKZFVJUW5y#/fill).
- **Fair Competition**: No plagiarism, multiple accounts, or rule exploitation

### ğŸ¯ Competition Task

Participants' models must correctly answer various questions about test videos, covering:

- Content Understanding 
- Culture  
- Spatial Understanding
- Temporal Understanding
- Science & Techology
- Entertainment & Media
- Causal reasoning  
- Streaming Understanding
- Anomaly Detection

The input and output definitions for this task are as follows:

**Input**: Video data + QA pairs
**Output**: Answer choices for QA pairs

#### Example Format:
```json
{
  "question_id": "q1", 
  "video_path": "video1.mp4",
  "question": "è§†é¢‘ä¸­å¥³å­æ‰€ç©¿çš„è¿™ç§æœé¥°äºå“ªä¸€å¹´å¼€å§‹æµè¡Œï¼Ÿ",
  "options": ["1921", "1896", "1919", "1925"],
}
```

## ğŸ“Š Dataset

### Benchmark A
- **Videos**: 1,389 videos from YouTube, Bilibili, Xiaohongshu
- **QA Pairs**: 3,134 carefully annotated pairs
- **Annotation**: Generated by Qwen2-VL-72B, Qwen2.5-72B, DeepSeek-V3, DeepSeek-R1 with human verification

For more detailed information, please refer to the [VideoVista-CulturalLingo](https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo) dataset page. Each team is required to submit a single prediction file for the entire benchmark.

### Benchmark B
- **Videos**: 2,619 videos from diverse sources.

- **QA Pairs**: 3,515 annotated pairs.

- Compared with Benchmark A, Benchmark B includes more task types of questions.

- Comprehensive evaluation of video understanding, perception, and reasoning
- Answers not open-source to ensure fair evaluation
- Final evaluation is based on B Benchmark scores

## ğŸ“ˆ Evaluation Metric

**Accuracy (Acc)**:

Acc = Number of correct predictions / Total number of test samples


## ğŸ… Awards

| Prize | Teams | Award | Certificate |
|-------|--------|--------|-------------|
| ğŸ¥‡ First Prize | 1 team | Â¥10,000 | âœ… |
| ğŸ¥ˆ Second Prize | 1 team | Â¥6,000 | âœ… |
| ğŸ¥‰ Third Prize | 1 team | Â¥3,000 | âœ… |
| ğŸ… Excellence Award | 2 teams | Â¥2,000 | âœ… |
| ğŸ… Innovation Award | 1 team | Â¥2,000 | âœ… |

*All amounts are pre-tax. Teams are responsible for internal prize distribution.*

## âš–ï¸ Competition Rules

- **Fair Play**: No plagiarism, rule exploitation, or multiple account usage
- **Original Work**: All submissions must be original
- **Data Usage**: Competition data cannot be used for commercial purposes
- **Intellectual Property**: Participants retain IP rights to their solutions
- **Code Sharing**: Code and models cannot be open-sourced during competition

## ğŸ› ï¸ Baseline Models

Participants must build on **open-source models** with **no more than 30B** parameters when developing architectural innovations and training methods.

Teams are encouraged to perform fine-tuning, architectural modifications, and innovative training approaches on these base models.

Agents are also allowed, provided that the total parameter count remains within the same 30B limit.

We recommend the following Uni-MoE 2.0 as a baseline.

- [**Uni-MoE 2.0**](https://huggingface.co/HIT-TMG/Uni-MoE-2.0-Base)



## ğŸ“¤ Submission

### Submission Process
- **Method**: Email submission **(Benchmark A limited to 4 submissions per week, Benchmark B limited to 8 submissions per week)**
- **Early Phase**: Submit only prediction results files
- **Final Phase**: Submit model weights for inference verification

### Submission JSON Format Example

```json
[
  {
    "question_id": "question_0",
    "prediction": "D"
  },
  {
    "question_id": "question_1", 
    "prediction": "B"
  },
   ...
]
```

### Submission Email
- **Address**: videovista@163.com
- **Subject**: TeamName_Submission_Date (Example: TMG_Submission_2025-11-17)
- **Attachment**: JSON file with predictions

## ğŸ“Š Leaderboard

### Benchmark A

| Rank | Team |  Overall | Last Update |
|------|------|---------|-------------|
| 1 | FAITA | 72.02 | 2026-01-04 |
| 2 | SeekingYourRoots | 64.90 | 2025-12-29 |
| 3 | vid | 60.31 | 2025-12-25 |
| 4 | JNU_BenchTest | 53.45 | 2025-12-18 |
| 5 | VILAN | 41.80 | 2026-01-03 |


### Benchmark B

| Rank | Team | Overall | Last Update |
|------|-------|---------|-------------|
| 1 |  Baseline-Qwen3-VL-8B-Instruct | 59.26 | 2025-11-17 |
| 2 |  Baseline-Uni-MoE-2.0-Omni | 49.62 | 2025-11-17 |
| 3 | Baseline-InternVL3_5-8B | 47.08 | 2025-11-17 |


*Leaderboard updates every 24 hours during competition period*

## ğŸ’ Sponsors

Thanks to Huawei Cloud for sponsoring this competition!

<!-- <div align="center">
  <img src="assets/huawei.svg" alt="Huawei Logo" width="200"/>
</div> -->


## ğŸ“š Citation

If you find this project useful in your research, please consider citing our papers:

```bibtex
@article{li2024videovista,
  title={Videovista: A versatile benchmark for video understanding and reasoning},
  author={Li, Yunxin and Chen, Xinyu and Hu, Baotian and Wang, Longyue and Shi, Haoyuan and Zhang, Min},
  journal={arXiv preprint arXiv:2406.11303},
  year={2024}
}

@article{chen2025videovista,
  title={VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension},
  author={Chen, Xinyu and Li, Yunxin and Shi, Haoyuan and Hu, Baotian and Luo, Wenhan and Wang, Yaowei and Zhang, Min},
  journal={arXiv preprint arXiv:2504.17821},
  year={2025}
}
```

## ğŸ¤ Contact & Support

For competition-related questions and support:

- **Email**: videovista@163.com
- **GitHub Issues**: Use GitHub Issues for technical questions and bug reports
- **Discussion Group**: Join our WeChat group for real-time updates and discussions

<div align="center">
  <img src="assets/wechat_0104.jpg" alt="WeChat Group" width="200"/>
</div>



## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.
