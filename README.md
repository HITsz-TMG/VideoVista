
<div align="center">
  <img src="VideoVista-CulturalLingo/asset/VideoVista_no_bg_s.png" alt="VideoVista Logo" width="300"/>
</div>

#  VideoVista Family



<a src="https://img.shields.io/badge/cs.CV-2406.11303-b31b1b?logo=arxiv&logoColor=red" href="https://arxiv.org/abs/2406.11303"> <img src="https://img.shields.io/badge/cs.CV-2406.11303-b31b1b?logo=arxiv&logoColor=red">
</a> | 


If you appreciate our project, please consider giving us a star ‚≠ê on GitHub to stay updated with the latest developments.

## üî• News

**`2025.04.15`** üöÄ We release VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. You can download this benchmark from [HuggingFace](https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo).

**`2025.04.13`** üéâ We move the previous VideoVista from [Uni-MoE](https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs) to here. It contains the [VideoVista (Evaluation), VideoVista-Train (Instruction Tuning), VideoVista-Event (Pertaining)](https://huggingface.co/collections/Uni-MoE/videovista-67f397b7eddc81cb207228a2)




## :page_facing_up: Citation
If you find this project useful in your research, please consider cite:
```bibtex
@misc{li2024videovista,
      title={VideoVista: A Versatile Benchmark for Video Understanding and Reasoning}, 
      author={Yunxin Li and Xinyu Chen and Baotian Hu and Longyue Wang and Haoyuan Shi and Min Zhang},
      year={2024},
      eprint={2406.11303},
      archivePrefix={arXiv}
}
```
