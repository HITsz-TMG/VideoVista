
<div align="center">
  <img src="VideoVista-CulturalLingo/asset/VideoVista_no_bg_s.png" alt="VideoVista Logo" width="300"/>
</div>

#  VideoVista Family



<a src="https://img.shields.io/badge/cs.CV-2406.11303-b31b1b?logo=arxiv&logoColor=red" href="https://arxiv.org/abs/2406.11303"> <img src="https://img.shields.io/badge/cs.CV-2406.11303-b31b1b?logo=arxiv&logoColor=red">
</a> | 
<a src="https://img.shields.io/badge/cs.CV-2406.11303-b31b1b?logo=arxiv&logoColor=red" href="https://arxiv.org/abs/2504.17821"> <img src="https://img.shields.io/badge/cs.CV-2406.11303-b31b1b?logo=arxiv&logoColor=red">
</a>

If you appreciate our project, please consider giving us a star ‚≠ê on GitHub to stay updated with the latest developments.

## üî• News

**`2025.04.23`** üöÄ We release VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. You can download this benchmark from [HuggingFace](https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo).

**`2025.04.13`** üéâ We move the previous VideoVista from [Uni-MoE](https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs) to here. It contains the [VideoVista (Evaluation), VideoVista-Train (Instruction Tuning), VideoVista-Event (Pertaining)](https://huggingface.co/collections/Uni-MoE/videovista-67f397b7eddc81cb207228a2). Detailed content is [here](https://github.com/HITsz-TMG/VideoVista/tree/main/VideoVista).




## :page_facing_up: Citation
If you find this project useful in your research, please consider cite:
```bibtex
@misc{li2024videovista,
        title={VideoVista: A Versatile Benchmark for Video Understanding and Reasoning}, 
        author={Yunxin Li and Xinyu Chen and Baotian Hu and Longyue Wang and Haoyuan Shi and Min Zhang},
        year={2024},
        eprint={2406.11303},
        archivePrefix={arXiv}
}

@misc{chen2025videovistaculturallingo,
      title={VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension}, 
      author={Xinyu Chen and Yunxin Li and Haoyuan Shi and Baotian Hu and Wenhan Luo and Yaowei Wang and Min Zhang},
      year={2025},
      eprint={2504.17821},
      archivePrefix={arXiv},
}
```
